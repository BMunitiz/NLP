{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d374127-32ca-4f40-90d6-c1a4f0382723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b6fe10-0be8-4353-8363-5e1e176597d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set environment variable for Keras backend to use TensorFlow\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  \n",
    "#import tensorflow_datasets as tfds\n",
    "# Import required libraries for NLP tasks\n",
    "import tensorflow_text\n",
    "import keras_nlp\n",
    "import keras\n",
    "import time\n",
    "# Enable mixed precision training to improve performance and reduce memory usag\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960e2e15-3830-4ca1-9d33-df8064ec27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warning messages to keep output clean\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e2db8a-6ffc-4eae-b28d-41379aa0d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up training and generation, we use preprocessor of length 128\n",
    "# instead of full length 1024.\n",
    "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "    \"gpt2_base_en\",\n",
    "    sequence_length=128,\n",
    ")\n",
    "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
    "    \"gpt2_base_en\", preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa851d1-43fa-4346-bcb1-cf8b64c6c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739958663.926486 16202430 service.cc:148] XLA service 0x600003a6b500 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1739958663.926536 16202430 service.cc:156]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1739958663.931689 16202430 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 output:\n",
      "My trip to Yosemite was pretty much the same as the last time. It was pretty much the same, except I had a few more things to do. I had to take the bus back to the airport, and I had to take a short drive back to the hotel to pick up my bags, and to get my passport back to the airport.\n",
      "\n",
      "I was really lucky that I got to go back to the airport, and to have a few hours with my family. I was really happy to see my mom and dad. They were really happy to see me, and really nice people to meet. I think I had a lot of fun with my family, and I'm really glad that I did.\n",
      "\n",
      "I was going to go to see my dad for his birthday, but he was going to have to go to his house, and then I was going to get a little more of an early morning break to get some time to relax and get some work done on the trail\n",
      "TOTAL TIME ELAPSED: 20.47s\n"
     ]
    }
   ],
   "source": [
    "# Record the start time for performance measurement\n",
    "start = time.time()\n",
    "\n",
    "# Generate text using GPT-2 model with the prompt \"My trip to Yosemite was\"\n",
    "# Set maximum length of generated text to 200 tokens\n",
    "output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "# Record end time and calculate total execution time\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0b7e07-4958-4a1d-b655-9c8ff6d99340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 output:\n",
      "That Italian restaurant is called \"The Italian Restaurant\", which is a name that comes from its Italian origins: \"The Italian Restaurant\". The restaurant is a place for Italians to enjoy their favorite dishes, to eat and enjoy. It is a great place to meet other Italian and other European travelers who want to learn about Italy.\n",
      "\n",
      "The Italian restaurant is open every day, from 5:00 pm to 7:00 pm and has a great view of the sea. The restaurant is open for lunch and dinner. The Italian restaurant is open for dinner every day from 7 pm to 10 pm.\n",
      "\n",
      "The restaurant has a great view, great atmosphere, and is very clean. It is a great place to stay for the evening. The Italian restaurant is a great place to stay for the evening. It is a great place to stay for the evening.\n",
      "TOTAL TIME ELAPSED: 15.98s\n"
     ]
    }
   ],
   "source": [
    "# Record the start time for performance measurement\n",
    "start = time.time()\n",
    "\n",
    "# Generate text using GPT-2 model with the given prompt and maximum length\n",
    "output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n",
    "# Print the generated output\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "# Record end time and calculate total execution time\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9a140-3996-41a2-b053-aed3362fea0e",
   "metadata": {},
   "source": [
    "## GPT text generation from scratch with KerasHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1836977-c44b-4ee5-88a3-a68b19dc751f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os                           # Operating system interface\n",
    "import keras_hub                    # TensorFlow Hub for Keras models\n",
    "import keras                        # Deep learning framework\n",
    "import tensorflow.strings as tf_strings  # TensorFlow string operations module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e73792-879b-49ca-aea3-850f7f305d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "BATCH_SIZE = 64  # Number of samples processed in each training iteration\n",
    "MIN_STRING_LEN = 512  # Minimum length threshold - strings shorter than this will be discarded\n",
    "SEQ_LEN = 128  # Length of training sequences measured in tokens\n",
    "\n",
    "# Model Architecture Parameters\n",
    "EMBED_DIM = 256  # Dimension of token embeddings\n",
    "FEED_FORWARD_DIM = 128  # Dimension of feed forward network in transformer\n",
    "NUM_HEADS = 3  # Number of attention heads in transformer\n",
    "NUM_LAYERS = 2  # Number of transformer encoder layers\n",
    "VOCAB_SIZE = 5000  # Maximum vocabulary size to limit model parameters\n",
    "\n",
    "# Training Configuration\n",
    "EPOCHS = 5  # Number of complete passes through the training dataset\n",
    "\n",
    "# Generation/Inference Settings  \n",
    "NUM_TOKENS_TO_GENERATE = 80  # Number of tokens to generate during text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e967c8e-89c6-49f9-98c0-303391252365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\n",
      "\u001b[1m282386239/282386239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Download and extract dataset from AWS S3 bucket\n",
    "# The dataset 'simplebooks.zip' contains text data for NLP tasks\n",
    "dir = keras.utils.get_file(\n",
    "    origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n",
    "    extract=True,  # Automatically extract the downloaded zip file\n",
    "    cache_dir= \"/Portfolio/NLP\"  # Local directory to store the dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c748f070-0d16-4b7b-9a56-6741c5c806c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a raw training dataset from text file\n",
    "raw_train_ds = (\n",
    "    # Load text data from file using TextLineDataset\n",
    "    tf.data.TextLineDataset( \"datasets/simplebooks.zip/simplebooks/simplebooks-92-raw/train.txt\")\n",
    "    # Filter out strings shorter than MIN_STRING_LEN\n",
    "    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n",
    "    # Batch the data into groups of BATCH_SIZE\n",
    "    .batch(BATCH_SIZE)\n",
    "    # Randomly shuffle batches with a buffer of 256 samples\n",
    "    .shuffle(buffer_size=256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e72881bd-a2bd-4f91-ac43-88061149325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation dataset from text file\n",
    "raw_val_ds = (\n",
    "    # Load text file from zip archive using TextLineDataset\n",
    "    tf.data.TextLineDataset(\"datasets/simplebooks.zip/simplebooks/simplebooks-92-raw/valid.txt\")\n",
    "    # Filter out strings shorter than MIN_STRING_LEN\n",
    "    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n",
    "    # Batch the data into groups of size BATCH_SIZE\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "368195e3-56dc-4046-8080-a688f2f6cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 11:06:20.921039: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Train a WordPiece tokenizer vocabulary from the training dataset\n",
    "# WordPiece is a subword tokenization algorithm that breaks words into smaller pieces\n",
    "vocab = keras_hub.tokenizers.compute_word_piece_vocabulary(\n",
    "    raw_train_ds,                    # Input training dataset\n",
    "    vocabulary_size=VOCAB_SIZE,      # Maximum size of vocabulary\n",
    "    lowercase=True,                  # Convert all text to lowercase\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],  # Special tokens for padding, unknown words, and beginning of sequence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb44b9cd-6ada-4025-b02c-746c73c6905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WordPieceTokenizer with specified parameters\n",
    "# - Uses provided vocabulary for tokenization\n",
    "# - Sets maximum sequence length to SEQ_LEN\n",
    "# - Converts all text to lowercase\n",
    "tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b0f42a05-84e3-4aa7-941f-0c8d525864ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.data as tf_data\n",
    "\n",
    "# Initialize packer to add [BOS] (beginning of sequence) token at start of sequences\n",
    "start_packer = keras_hub.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LEN,\n",
    "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(inputs):\n",
    "    # Convert input text to token IDs using tokenizer\n",
    "    outputs = tokenizer(inputs)\n",
    "    # Add start token to create features for training\n",
    "    features = start_packer(outputs)\n",
    "    # Use original tokenized sequence as labels\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Create training dataset by:\n",
    "# 1. Applying preprocessing to raw data\n",
    "# 2. Enabling parallel processing with AUTOTUNE\n",
    "# 3. Prefetching next batch while current batch processes\n",
    "train_ds = raw_train_ds.map(preprocess, num_parallel_calls=tf_data.AUTOTUNE).prefetch(\n",
    "    tf_data.AUTOTUNE\n",
    ")\n",
    "# Create validation dataset with same preprocessing pipeline\n",
    "val_ds = raw_val_ds.map(preprocess, num_parallel_calls=tf_data.AUTOTUNE).prefetch(\n",
    "    tf_data.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c97dd034-da32-4046-8a63-fa5b65de8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layer for variable length sequences of integers\n",
    "inputs = keras.layers.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Create embedding layer that combines token and positional embeddings\n",
    "embedding_layer = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,    # Size of the vocabulary\n",
    "    sequence_length=SEQ_LEN,       # Maximum sequence length\n",
    "    embedding_dim=EMBED_DIM,       # Dimension of embeddings\n",
    "    mask_zero=True,               # Enable masking for variable length sequences\n",
    ")\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "# Stack multiple transformer decoder layers\n",
    "for _ in range(NUM_LAYERS):\n",
    "    decoder_layer = keras_hub.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,           # Number of attention heads\n",
    "        intermediate_dim=FEED_FORWARD_DIM,  # Dimension of feed forward network\n",
    "    )\n",
    "    # Apply self-attention only (no cross-attention)\n",
    "    x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
    "\n",
    "# Final dense layer to project to vocabulary size\n",
    "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "perplexity = keras_hub.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5fadf68b-7338-4744-bc93-dea98bdbce05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">329,085</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_decoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">329,085</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m1,312,768\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m329,085\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_decoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m329,085\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m)     │     \u001b[38;5;34m1,285,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,255,938</span> (12.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,255,938\u001b[0m (12.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,255,938</span> (12.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,255,938\u001b[0m (12.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a summary of the model's architecture, including:\n",
    "# - Layer types and names\n",
    "# - Output shapes\n",
    "# - Number of parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2f4c79c-349d-4d08-8700-d2ee12a16df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2373s\u001b[0m 968ms/step - loss: 5.0178 - perplexity: 185.3096 - val_loss: 4.2258 - val_perplexity: 68.5013\n",
      "Epoch 2/5\n",
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2103s\u001b[0m 858ms/step - loss: 4.1704 - perplexity: 64.8094 - val_loss: 4.0732 - val_perplexity: 58.8164\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 12:22:38.913060: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5773s\u001b[0m 2s/step - loss: 4.0298 - perplexity: 56.2870 - val_loss: 4.0089 - val_perplexity: 55.1365\n",
      "Epoch 4/5\n",
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8397s\u001b[0m 3s/step - loss: 3.9550 - perplexity: 52.2264 - val_loss: 3.9710 - val_perplexity: 53.1024\n",
      "Epoch 5/5\n",
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2090s\u001b[0m 853ms/step - loss: 3.9084 - perplexity: 49.8464 - val_loss: 3.9409 - val_perplexity: 51.5549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3649597c0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the training dataset and validate using validation dataset\n",
    "# train_ds: Training dataset\n",
    "# val_ds: Validation dataset\n",
    "# epochs: Number of complete passes through the training dataset\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c9b4de76-0f7b-4326-adfb-889b7bb7c022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The \"packer\" layers adds the [BOS] token for us.\n",
    "# Create tokenized prompt by passing empty string to tokenizer and wrapping in list\n",
    "# Apply start_packer to add BOS token to tokenized prompt\n",
    "prompt_tokens = start_packer(tokenizer([\"\"]))\n",
    "# Display the resulting prompt tokens\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "78514f0e-2374-4cb3-ab79-de094c9cc56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next(prompt, cache, index):\n",
    "    # Get model predictions (logits) for the token at position (index-1)\n",
    "    # Shape: [batch_size, vocab_size]\n",
    "    logits = model(prompt)[:, index -1 , :]\n",
    "    \n",
    "    # Skip hidden state handling for now since it's only used in contrastive search\n",
    "    #cache = True\n",
    "    hidden_states = None\n",
    "    \n",
    "    # Return logits for next token prediction, along with empty hidden states and cache\n",
    "    return logits, hidden_states, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fef37b-c43f-4e1b-9dca-eff62f618c24",
   "metadata": {},
   "source": [
    "### Greedy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "563964de-419d-431f-9aa6-70e99b5948af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy search generated text: \n",
      "['[BOS] \" i \\' m not going to be a bit like a bit , \" said the doctor , \" and i \\' ll take a little girl \\' s to - day , and i \\' ll take a walk with me , and i \\' ll take a walk with me , and i \\' ll take a walk to the house . i \\' ll get a good old woman , and i \\' ll be a good old woman , and i \\' ll be a good old woman , and i \\' ll get a good woman , and i \\' ll get a good woman , and i \\' ll get a good woman , and i \\' ll get a']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a greedy sampler for text generation\n",
    "sampler = keras_hub.samplers.GreedySampler()\n",
    "\n",
    "# Generate output tokens using the sampler\n",
    "# - next: function that predicts next token probabilities\n",
    "# - prompt_tokens: input sequence to start generation from\n",
    "# - index=1: start sampling after the [BOS] (beginning of sequence) token\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,  # Start sampling immediately after the [BOS] token.\n",
    ")\n",
    "\n",
    "# Convert the generated tokens back to readable text\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "\n",
    "# Print the generated text\n",
    "print(f\"Greedy search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03638e71-df34-4f9e-ad63-7d45b59fa968",
   "metadata": {},
   "source": [
    "### Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6bd81b2e-9e88-4ba4-8b35-6e7d0cfc4c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam search generated text: \n",
      "['[BOS] \" yes , sir , \" he said , \" but i don \\' t know how much more than i am . but i don \\' t know what i am going to tell you about it . but i don \\' t know what it is , and i don \\' t know it , but i don \\' t know about it , and i don \\' t know about it , but i don \\' t know it , and i \\' ll tell you about it , and i \\' ll tell you about it , and i \\' ll tell you about it , and i \\' ll tell you about it , and i \\' ll tell you , \\'']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize beam search sampler with 10 beams for text generation\n",
    "sampler = keras_hub.samplers.BeamSampler(num_beams=10)\n",
    "\n",
    "# Generate output tokens using beam search\n",
    "# - next: function that predicts next tokens\n",
    "# - prompt_tokens: input sequence to start generation from\n",
    "# - index: position to start generating from\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "\n",
    "# Convert generated tokens back to readable text\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "\n",
    "# Print the generated text\n",
    "print(f\"Beam search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb853a-91f3-40da-8446-477999f1ea32",
   "metadata": {},
   "source": [
    "### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c688044d-af4e-4e88-ac9f-3da6842877bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random search generated text: \n",
      "['[BOS] it was dry to put onto a diary point , for he saw the four long windows , and let me get a board on the walls . he pulled the first paper , and spoke to the bill a quarter in the manner in which glarest fericks fell , and when they fell back for him to wait him . \" it reached just instead of eleven , let its testim \\' s roll , and follow me out . \" in my piece , it travelled so , but at one o \\' margin , an \\' presently she ran with a pair of eyes flying out like a snake , sometimes he']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a random sampler for text generation\n",
    "sampler = keras_hub.samplers.RandomSampler()\n",
    "\n",
    "# Generate output tokens using the sampler\n",
    "# - next: function to get next token probabilities\n",
    "# - prompt: input token sequence\n",
    "# - index: starting position for generation\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "\n",
    "# Convert the generated tokens back to readable text\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "\n",
    "# Print the randomly generated text\n",
    "print(f\"Random search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733bfe3b-ed5b-46ad-826b-f94a2716f1e8",
   "metadata": {},
   "source": [
    "### Top-K search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7e544996-b1bf-4160-8db6-7ba0ffbd3852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-K search generated text: \n",
      "[\"[BOS] now , then , when they were ready to take up a little chuckling , the old - house was empty and there were some chatterer of apples . then , as a farmer brown ' s boy , and the old man , who had been so hungry , could not be eaten with , they ate them , and the old woman was quite sure that she was all the old woman ' s house was in the old woman ' s room with her . the old woman , was very much surprised to see her and said : ' now you ' d better have to come home . ' and she said , ' no .\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Top-K sampler with k=10 (selects from top 10 most likely tokens)\n",
    "sampler = keras_hub.samplers.TopKSampler(k=10)\n",
    "\n",
    "# Generate tokens using the sampler with:\n",
    "# - next: token generation function\n",
    "# - prompt_tokens: initial input sequence\n",
    "# - index: starting position (1)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "\n",
    "# Convert the generated tokens back to readable text\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "\n",
    "# Print the generated text with Top-K sampling\n",
    "print(f\"Top-K search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c45ea-d8ae-4746-b8f1-c1eee5f2fdac",
   "metadata": {},
   "source": [
    "### Top-P search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c61d7ec7-93ad-4c1d-b9ef-c784ed4e3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-P search generated text: \n",
      "['[BOS] \" i \\' m glad , \" she said . \" the water is hot , and there \\' s transport . \" i \\' m not going to get some breakfast , and we \\' ll take a comparel . you \\' ll find a rout disappearance , and you \\' ll take a part with me , and i \\' ll give a little trouble to put up to a cheese , \\' and then you \\' ll get on to - night , \\' cause he can \\' t a bit . \" he was a very bad boy , an \\' he thought he was a little in']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Top-P (nucleus) sampler with probability threshold of 0.5\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.5)\n",
    "\n",
    "# Generate tokens using the sampler\n",
    "# next: token generation function\n",
    "# prompt_tokens: input sequence of tokens\n",
    "# index: position to start generating from\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "\n",
    "# Convert generated tokens back to readable text\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "\n",
    "# Print the generated text\n",
    "print(f\"Top-P search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbada4ad-49df-40a6-a4ea-1a5c1a5d6270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLPKeras]",
   "language": "python",
   "name": "conda-env-NLPKeras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
